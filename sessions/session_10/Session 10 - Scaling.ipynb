{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course: DevOps, Maintenance, and Evolution\n",
    "\n",
    "# Session: Scaling \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The state of your projects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600\"\n",
       "            src=\"http://64.225.103.230/commit_activity_daily.svg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10639ec18>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "url = 'http://64.225.103.230/commit_activity_daily.svg'\n",
    "IFrame(url, width='100%', height='600') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500\"\n",
       "            src=\"http://142.93.104.18/chart.svg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10639eb70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "url = 'http://142.93.104.18/chart.svg'\n",
    "IFrame(url, width='100%', height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500\"\n",
       "            src=\"http://142.93.104.18/error_chart.svg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10639e978>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "url = 'http://142.93.104.18/error_chart.svg'\n",
    "IFrame(url, width='100%', height=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How is the SLA monitoring going?\n",
    "\n",
    "- Which teams found their monitored teams to respect the SLAs? Partially?\n",
    "- How is your team doing? Your monitored team? \n",
    "- What did you learn about writing SLAs?\n",
    "\n",
    "\n",
    "\n",
    "## Pen-Testing \n",
    "\n",
    "- Did you succeed in finding vulnerabilities? Either your code or the other teams?\n",
    "- Make sure to send your report to the tested group :)\n",
    "\n",
    "## Logging\n",
    "- Could you detect any attempts on your service?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bottlenecks appear in any good design\n",
    "\n",
    "Most Common Reasons:\n",
    "  * Congestion\n",
    "  * Single point of failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Single Point of Failure\n",
    "\n",
    "Very likely your systems look similar to the following:\n",
    "\n",
    "![](images/possible_arch1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Single Point of Failure\n",
    " \n",
    "Or perhaps you deployed the various components on their own machines:\n",
    "![](images/possible_arch2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What happens if one of the nodes goes down?\n",
    "\n",
    "- every one of the systems is a single point of failure\n",
    "- the solution: replication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Solution to Congestion - Scaling\n",
    "\n",
    "Two Types of Scaling:\n",
    "- Vertical\n",
    "- Horizontal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vertical Scaling - Manually\n",
    "\n",
    "  * In a physical server add more memory, harddisk etc.\n",
    "  * Nice thing about using VMs, we can do that at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " \n",
    "## Vertical Scaling with VirtualBox\n",
    "\n",
    "Note the VM has to be powered off to perform modifications of RAM and storage. You can modify VMs via the GUI or on the CLI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GUI\n",
    "\n",
    "![](https://i.stack.imgur.com/IEtuL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://i.stack.imgur.com/AYfo9.png)\n",
    "\n",
    "See https://askubuntu.com/a/510139"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### CLI\n",
    "\n",
    "To increase RAM of an existing VM to 8GB, storage to 64GB, and the CPU to more cores, one could run:\n",
    "\n",
    "```bash\n",
    "> VBoxManage list vms\n",
    "\"coursevm\" {e072b310-1922-4113-93f5-2ca865e01722}\n",
    "\"lsd2018vm\" {67fda2ea-7c3e-42f6-9a13-d0908020322d}\n",
    "```\n",
    "\n",
    "```bash\n",
    "> VBoxManage modifyvm \"coursevm\" --cpus 8\n",
    "> VBoxManage modifyvm \"coursevm\" --memory 8192\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```bash\n",
    "> VBoxManage list hdds\n",
    "UUID:           d95b799f-0f33-4abd-925c-84ea5cbb3225\n",
    "Parent UUID:    base\n",
    "State:          created\n",
    "Type:           normal (base)\n",
    "Location:       /path/to/node1/ubuntu-16.04-amd64-disk001.vmdk\n",
    "Storage format: VMDK\n",
    "Capacity:       40960 MBytes\n",
    "Encryption:     disabled\n",
    "\n",
    "UUID:           9953ea1b-7295-4547-94fa-209f49c258f5\n",
    "Parent UUID:    base\n",
    "State:          created\n",
    "Type:           normal (base)\n",
    "Location:       /path/to/node1ubuntu-16.04-amd64-disk001.vmdk\n",
    "Storage format: VMDK\n",
    "Capacity:       40960 MBytes\n",
    "Encryption:     disabled\n",
    "```\n",
    "\n",
    "```sh\n",
    "> VBoxManage clonehd \"9953ea1b-7295-4547-94fa-209f49c258f5\" \"cloned.vdi\" --format vdi\n",
    "> VBoxManage modifymedium disk \"cloned.vdi\" --resize 65536\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Optionally, you may want to convert the disk back to `vmdk` with `VBoxManage clonehd \"cloned.vdi\" \"resized.vmdk\" --format vmdk`\n",
    "\n",
    "**OBS** from within your VM make use of the new space by extending your disk with `gparted` or similar tools.\n",
    "\n",
    "See:\n",
    "  * Modfication of RAM https://www.virtualbox.org/manual/ch08.html#vboxmanage-modifyvm \n",
    "  * Modification of storage https://www.virtualbox.org/manual/ch08.html#vboxmanage-modifyvdi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vertical Scaling with DigitalOcean\n",
    "\n",
    "\n",
    "### GUI\n",
    "\n",
    "\n",
    "![](https://assets.nyc3.cdn.digitaloceanspaces.com/droplets/resize-options.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But -similar to VirtualBox- you can only resize VMs that are not running.\n",
    "\n",
    "![](https://assets.nyc3.cdn.digitaloceanspaces.com/droplets/resize-switch-on.png)\n",
    "\n",
    "\n",
    "\n",
    "See https://www.digitalocean.com/docs/droplets/how-to/resize/#resizing-via-the-control-panel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### CLI\n",
    "\n",
    "Using the HTTP API vertical scaling could look as in the following.\n",
    "\n",
    "See \n",
    "\n",
    "  * https://www.digitalocean.com/docs/droplets/how-to/resize/\n",
    "  * https://developers.digitalocean.com/documentation/v2/#resize-a-droplet\n",
    "\n",
    "\n",
    "**Note**: The following commands assume that you have defined the $DIGITAL_OCEAN_TOKEN envvar.\n",
    "\n",
    "\n",
    "##### Create a New Droplet\n",
    "\n",
    "```bash\n",
    "> curl -X POST \"https://api.digitalocean.com/v2/droplets\" \\\n",
    "       -d'{\"name\":\"New-Droplet\",\"region\":\"fra1\",\"size\":\"512mb\",\"image\":\"ubuntu-16-04-x64\"}' \\\n",
    "       -H \"Authorization: Bearer $DIGITAL_OCEAN_TOKEN\" \\\n",
    "\t   -H \"Content-Type: application/json\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### List all Droplets\n",
    "\n",
    "```sh\n",
    "> curl -X GET \"https://api.digitalocean.com/v2/droplets\" -H \"Authorization: Bearer $DIGITAL_OCEAN_TOKEN\" | jq\n",
    "\n",
    "{\n",
    "  \"droplets\": [\n",
    "    {\n",
    "      \"id\": 116690982,\n",
    "      \"name\": \"webserver\",\n",
    "      \"memory\": 1024,\n",
    "      \"vcpus\": 1,\n",
    "      \"disk\": 30,\n",
    "      \"locked\": false,\n",
    "      \"status\": \"active\",\n",
    "      \"kernel\": null,\n",
    "      \"created_at\": \"2018-10-26T12:54:08Z\",\n",
    "      \"features\": [],\n",
    "...\n",
    "      \"networks\": {\n",
    "        \"v4\": [\n",
    "          {\n",
    "            \"ip_address\": \"46.101.225.71\",\n",
    "            \"netmask\": \"255.255.192.0\",\n",
    "            \"gateway\": \"46.101.192.1\",\n",
    "            \"type\": \"public\"\n",
    "          }\n",
    "        ],\n",
    "        \"v6\": []\n",
    "      },\n",
    "...\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Resizing it\n",
    "\n",
    "Resize CPU and RAM only to a 4GB droplet, which automatically shuts down the droplet. \n",
    "\n",
    "```sh\n",
    "> curl -X POST -H 'Content-Type: application/json' \\\n",
    "       -H 'Authorization: Bearer $DIGITAL_OCEAN_TOKEN' \\\n",
    "       -d '{\"type\":\"resize\",\"size\":\"s-2vcpu-4gb\"}' \\\n",
    "       \"https://api.digitalocean.com/v2/droplets/$DROPLET_ID/actions\"\n",
    "```\n",
    "\n",
    "what's wrong with this?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "##### Increase disk size:\n",
    "\n",
    "\n",
    "```sh\n",
    "> curl -X POST -H 'Content-Type: application/json' \\\n",
    "       -H \"Authorization: Bearer $DIGITAL_OCEAN_TOKEN\" \\\n",
    "       -d '{\"type\":\"resize\",\"size\": \"s-2vcpu-4gb\",\"disk\":true}'  \\\n",
    "       \"https://api.digitalocean.com/v2/droplets/$DROPLET_ID/actions\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Power on\n",
    "\n",
    "```sh\n",
    "> curl -X POST -H 'Content-Type: application/json' \\\n",
    "       -H \"Authorization: Bearer $DIGITAL_OCEAN_TOKEN\" \\\n",
    "       -d '{\"type\":\"power_on\"}' \\\n",
    "       \"https://api.digitalocean.com/v2/droplets/$DROPLET_ID/actions\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Discussion\n",
    "\n",
    "Now that is quite cool no? You could attach a resize action to an alarm and scale you machines vertically automatically.\n",
    "\n",
    "But what is the drawback? You have to switch off the machines that you want to scale. That is quite unpractical if you have do not have reduntant machines, see more about that below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizontal Scaling\n",
    "\n",
    "\n",
    "Horizontal scaling means not increasing the resources of a single (virtual) machine but instead adding more (virtual) machines to your cluster.\n",
    "\n",
    "![](https://assets.digitalocean.com/blog/static/scaling-php/horizontal-scaling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Horizontal Scaling does not Only Solve Congestion but also Single Point of Failure\n",
    "\n",
    "Note: probably change the title to match the next slide :)\n",
    "\n",
    "![](images/possible_arch2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Redundancy/High-available Setup\n",
    "\n",
    "![](https://assets.digitalocean.com/blog/static/floating-ips-start-architecting-your-applications-for-high-availability/ha-diagram-animated.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For guides on manual horizontal scaling on DigitalOcean see for example:\n",
    "\n",
    "  * https://blog.digitalocean.com/floating-ips-start-architecting-your-applications-for-high-availability/\n",
    "  * https://www.digitalocean.com/community/tutorials/how-to-set-up-highly-available-web-servers-with-keepalived-and-floating-ips-on-ubuntu-14-04\n",
    "  * https://www.digitalocean.com/community/tutorials/how-to-create-a-high-availability-setup-with-heartbeat-and-floating-ips-on-ubuntu-16-04\n",
    "  * https://blog.digitalocean.com/load-balancers-simplifying-high-availability/\n",
    "  * https://blog.digitalocean.com/horizontally-scaling-php-applications/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Tools that Help Horizontal Scaling\n",
    "\n",
    "  * Kubernetes, the elephant\n",
    "  * Docker Swarm, the mouse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kubernetes\n",
    "\n",
    "  * ... is popular in the US, see https://trends.google.com/trends/explore?geo=US&q=Kubernetes,Docker%20Swarm,Mesos\n",
    "  * But since Danmark has another size, it is not that much more popular compared to alternatives, see\n",
    "  https://trends.google.com/trends/explore?geo=DK&q=Kubernetes,Docker%20Swarm,Mesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "![](images/full-kubernetes-model-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker Swarm\n",
    "\n",
    "We are going for the mouse...\n",
    "\n",
    "  * Based on `swarmkit`\n",
    "    > A toolkit for orchestrating distributed systems at any scale\n",
    "  * Orchestration\n",
    "    - Manages nodes and services\n",
    "  * Scheduling\n",
    "    - Resource aware task scheduling\n",
    "  \n",
    "Scalability?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Docker Swarm: Concepts\n",
    "\n",
    "  * Nodes\n",
    "    - A VM instance participating in a swarm \n",
    "    - Worker nodes\n",
    "    - Manager nodes\n",
    "  * Services and tasks\n",
    "    - A collection of tasks to be executed on a node, i.e., containers\n",
    "    - Replicated services\n",
    "    - Global services (one instance per node)\n",
    "  * Load balancing\n",
    "See https://docs.docker.com/engine/swarm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Docker Swarm: Nodes\n",
    "\n",
    "Two types of nodes: \n",
    "  * Managers\n",
    "  * Workers\n",
    "\n",
    "![](https://docs.docker.com/engine/swarm/images/swarm-diagram.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manager nodes\n",
    "\n",
    "  * maintain cluster state\n",
    "  * schedule services\n",
    "\n",
    "For teaching we run a single swarm manager. In production you should have at least three manager nodes.\n",
    "\n",
    "  * Three manager swarm tolerates a maximum loss of one manager.\n",
    "  * Five manager swarm tolerates a maximum loss of two managers.\n",
    "  * $n$ manager swarm tolerates maximum loss of $(n-1)/2$ managers.\n",
    "\n",
    "Docker recommends a maximum of seven manager nodes for a swarm.\n",
    "\n",
    "Important Note: Adding more managers does NOT mean increased scalability or higher performance. In general, the opposite is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Worker nodes\n",
    "\n",
    "  * are instances of Docker Engine whith sole purpose to execute containers\n",
    "  * do not participate in scheduling decisions, or serve the swarm mode HTTP API\n",
    "  * Have at least one manager node\n",
    "  * (By default, all managers are also workers)\n",
    "\n",
    "See more https://docs.docker.com/engine/swarm/how-swarm-mode-works/nodes/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://docs.docker.com/engine/swarm/images/replicated-vs-global.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|Swarm Size|\tMajority\t|Fault Tolerance|\n",
    "|:----------:|:-------------:|:------:|\n",
    "|1\t|1\t|0|\n",
    "|2\t|2\t|0|\n",
    "|3\t|2\t|1|\n",
    "|4\t|3\t|1|\n",
    "|5\t|3\t|2|\n",
    "|6\t|4\t|2|\n",
    "|7\t|4\t|3|\n",
    "|8\t|5\t|3|\n",
    "|9\t|5\t|4|\n",
    "\n",
    "\n",
    "https://docs.docker.com/engine/swarm/admin_guide/#add-manager-nodes-for-fault-tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## New Docker Commands\n",
    "\n",
    "  * `docker-machine` ... to manage virtual machines\n",
    "    - usually used for quickly creating new VMs\n",
    "  * `docker swarm` ... to manage a cluster (swarm)\n",
    "  * `docker service` ... to manage replicated containers (services) in the swarm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally Or on Other Providers than DO\n",
    "If you want to create such a node on your local machine with VirtualBox as backend:\n",
    "\n",
    "```sh\n",
    "$ docker-machine create --driver virtualbox default\n",
    "```\n",
    "\n",
    "See more on that in the documentation: https://docs.docker.com/machine/get-started/\n",
    "\n",
    "\n",
    "However, in the following we will use DigitalOcean for providing VMs for the nodes, see https://docs.docker.com/machine/drivers/digital-ocean/#usage\n",
    "\n",
    "If your project runs on another provider you may want to check the other available drivers:\n",
    "https://docs.docker.com/machine/drivers/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Interactive: Docker Swarm cluster on DigitalOcean!\n",
    "\n",
    "Create a Docker Swarm cluster on DigitalOcean (or locally with VirtualBox) with the following guide.\n",
    "\n",
    "Note: You need to have `docker-machine` installed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Docker Swarm Cluster Node\n",
    "\n",
    "\n",
    "```sh\n",
    "> export DIGITALOCEAN_REGION=fra1 \n",
    "```\n",
    "\n",
    "```sh\n",
    "> export DIGITALOCEAN_SIZE=1gb\n",
    "```\n",
    "\n",
    "```sh\n",
    "> export DIGITALOCEAN_PRIVATE_NETWORKING=true\n",
    "```\n",
    "\n",
    "```sh\n",
    "> docker-machine create --driver digitalocean \\\n",
    "                        --digitalocean-image ubuntu-18-04-x64 \\\n",
    "                        --digitalocean-access-token $DIGITAL_OCEAN_TOKEN \\\n",
    "                        node-0\n",
    "```\n",
    "\n",
    "... about 3min ... \n",
    "\n",
    "~~~bash\n",
    "> docker-machine ls\n",
    "NAME     ACTIVE   DRIVER         STATE     URL                         SWARM   DOCKER     ERRORS\n",
    "node-0   -        digitalocean   Running   tcp://142.93.109.102:2376           v18.09.0\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Creating Two More Nodes\n",
    "\n",
    "```bash\n",
    "> docker-machine create --driver digitalocean \\\n",
    "                        --digitalocean-image ubuntu-18-04-x64 \\\n",
    "                        --digitalocean-access-token $DIGITAL_OCEAN_TOKEN \\\n",
    "                        node-1\n",
    "```\n",
    "\n",
    "```sh\n",
    "> docker-machine create --driver digitalocean \\\n",
    "                        --digitalocean-image ubuntu-18-04-x64 \\\n",
    "                        --digitalocean-access-token $DIGITAL_OCEAN_TOKEN \\\n",
    "                        node-2\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Alternatively, you could create a cluster with multiple nodes via a loop:\n",
    "\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "export DIGITALOCEAN_REGION=fra1\n",
    "export DIGITALOCEAN_SIZE=1gb\n",
    "export DIGITALOCEAN_PRIVATE_NETWORKING=true\n",
    "for (( i=0; i<=2; i++ ))\n",
    "do\n",
    "    echo \"Creating node $i\"\n",
    "    docker-machine create --driver digitalocean \\\n",
    "                          --digitalocean-image ubuntu-18-04-x64 \\\n",
    "                          --digitalocean-access-token $DIGITAL_OCEAN_TOKEN \\\n",
    "                          node-$i\n",
    "done\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the IP of a Cluster Node\n",
    "\n",
    "~~~bash\n",
    "$ docker-machine ip node-0\n",
    "~~~\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making `node-0` a Cluster Manager\n",
    "\n",
    "```bash\n",
    "> SWARM_MANAGER_IP=`docker-machine ip node-0`\n",
    "```\n",
    "\n",
    "```sh\n",
    "> echo $SWARM_MANAGER_IP\n",
    "```\n",
    "\n",
    "```\n",
    "142.93.109.102\n",
    "```\n",
    "\n",
    "\n",
    "```sh\n",
    "\n",
    "> docker-machine ssh node-0 \"docker swarm init --advertise-addr $SWARM_MANAGER_IP\"\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "Swarm initialized: current node (sozjy3nmfrieacm2pbgj41ek3) is now a manager.\n",
    "\n",
    "To add a worker to this swarm, run the following command:\n",
    "\n",
    "    docker swarm join --token SWMTKN-1-4rndqz4hwe38wtbl9fwgj33rk48ok3hri7a0xy42o7sf5ll38z-afkri2vu57m5z31v34bny16aj 142.93.109.102:2377\n",
    "\n",
    "To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting node-1 and node-2 to Workers\n",
    "\n",
    "Now let's get that token from the remote node and build a command that we can run on node-1 and node-2 to join the swarm.\n",
    "\n",
    "```sh\n",
    "> docker-machine ssh node-0 \"docker swarm join-token worker -q\"\n",
    "SWMTKN-1-4rndqz4hwe38wtbl9fwgj33rk48ok3hri7a0xy42o7sf5ll38z-afkri2vu57m5z31v34bny16aj\n",
    "\n",
    "> MANAGER_TOKEN=`docker-machine ssh node-0 \"docker swarm join-token worker -q\"`\n",
    "\n",
    "> REMOTE_CMD=\"docker swarm join --token $MANAGER_TOKEN $SWARM_MANAGER_IP:2377\"\n",
    "\n",
    "> docker-machine ssh node-1 \"$REMOTE_CMD\"\n",
    "```\n",
    "\n",
    "```\n",
    "  This node joined a swarm as a worker.\n",
    "```\n",
    "\n",
    "```sh\n",
    "> docker-machine ssh node-2 \"$REMOTE_CMD\"\n",
    "```\n",
    "\n",
    "```\n",
    "  This node joined a swarm as a worker.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the manager, we can see the state of the cluster\n",
    "\n",
    "```sh\n",
    "\n",
    "> docker-machine ssh node-0 \"docker node ls\"\n",
    "\n",
    "ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION\n",
    "sozjy3nmfrieacm2pbgj41ek3 *   node-0              Ready               Active              Leader              18.09.0\n",
    "hy6ie5xq561f9w1zpiyaqkrk5     node-1              Ready               Active                                  18.09.0\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Starting a Service\n",
    "\n",
    "Now that everything is setup, let's run a service on our cluster:\n",
    "\n",
    "```sh\n",
    "> docker-machine ssh node-0 \"docker service create -p 8080:8080 --name appserver stifstof/crashserver\"\n",
    "overall progress: 0 out of 1 tasks\n",
    "...\n",
    "overall progress: 1 out of 1 tasks\n",
    "verify: Waiting 5 seconds to verify that tasks are stable...\n",
    "...\n",
    "verify: Waiting 1 seconds to verify that tasks are stable...\n",
    "verify: Service converged\n",
    "```\n",
    "\n",
    "... about 1-2 min ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Checking the State of the Service\n",
    "\n",
    "\n",
    "```bash\n",
    "> docker-machine ssh node-0 \"docker service ls\"\n",
    "ID                  NAME                MODE                REPLICAS            IMAGE                        PORTS\n",
    "ttkqm9wzthgu        appserver           replicated          1/1                 stifstof/crashserver:latest   *:8080->8080/tcp\n",
    "```\n",
    "\n",
    "You may directly ask for the state of a service with\n",
    "\n",
    "```bash\n",
    "> docker-machine ssh node-0 \"docker service ps appserver\"\n",
    "```\n",
    "\n",
    "\n",
    "Now, on a Mac you can: \n",
    "\n",
    "```sh\n",
    "> open http://$SWARM_MANAGER_IP:8080\n",
    "```\n",
    "\n",
    "Alternatively, navigate manually to the swarm manager's IP port 8080 and see the webpage served. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens if you navigate to the worker node ips? \n",
    "\n",
    "\n",
    "```sh\n",
    "open http://`docker-machine ip node-1`:8080\n",
    "```\n",
    "\n",
    "```sh\n",
    "open http://`docker-machine ip node-2`:8080\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Routing Mesh\n",
    "\n",
    "\n",
    "> The routing mesh enables each node in the swarm to accept connections on published ports for any service running in the swarm, even if there’s no task running on the node. The routing mesh routes all incoming requests to published ports on available nodes to an active container.\n",
    "\n",
    "\n",
    "Read more:  https://docs.docker.com/engine/swarm/ingress/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Swarm also Restarts Services\n",
    "\n",
    "\n",
    "To demonstrate this, we used the crashserver service - which is a webserver which kills itself three seconds after serving an http request.\n",
    "\n",
    "Take some time and observe the behavior of the container before continuing with the guide. \n",
    "Note how the infrastructure is self-healing, by checking the state of the service multiple times after an invocation as shown above.\n",
    "\n",
    "The service becomes unavailable while Swarm is recreating the container after it has been killed. \n",
    "Now we will scale the service to increase availability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Finally, Scaling\n",
    "\n",
    "```bash\n",
    "> docker-machine ssh node-0 \"docker service scale appserver=5\"\n",
    "> docker-machine ssh node-0 \"docker service ls\"\n",
    "\n",
    "ID                  NAME                MODE                REPLICAS            IMAGE                        PORTS\n",
    "ttkqm9wzthgu        appserver           replicated          5/5                 stifstof/crashserver:latest   *:8080->8080/tcp\n",
    "\n",
    "\n",
    "\n",
    "> docker-machine ssh node-0 \"docker service ps appserver\"\n",
    "\n",
    "\n",
    "ID                  NAME                IMAGE                        NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS\n",
    "vbg02o9bsaog        appserver.1         stifstof/crashserver:latest   node-1              Running             Running 7 minutes ago\n",
    "mudpe1lokpj7        appserver.2         stifstof/crashserver:latest   node-0              Running             Running 13 seconds ago\n",
    "t7enei6pz4jw        appserver.3         stifstof/crashserver:latest   node-0              Running             Running 12 seconds ago\n",
    "sfpn4f2kg5nq        appserver.4         stifstof/crashserver:latest   node-1              Running             Running 39 seconds ago\n",
    "wa8f99b6t199        appserver.5         stifstof/crashserver:latest   node-0              Running             Running 12 seconds ago\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Did it work? \n",
    "\n",
    "You should now be able to invoke the webpage without seeing the error-page each time the container is killed, but instead see the request being served by another container. Nice!\n",
    "\n",
    "Althrough it is possible to kill all container by manically invoking the /status endpoint, if you want to test the self-healing feature of swarm, you can invoke the /kill endpoint, which will kill the container immediately, so you don't have to wait.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Cleaning up to not pay anymore...\n",
    "\n",
    "\n",
    "\n",
    "```bash\n",
    "> docker-machine ls --filter name=node-.* --filter driver=digitalocean --format \"{{.Name}}\" | xargs docker-machine rm -y\n",
    "\n",
    "About to remove node-0, node-1\n",
    "WARNING: This action will delete both local reference and remote instance.\n",
    "Successfully removed node-0\n",
    "Successfully removed node-1\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "The guide above is based on the guide at:\n",
    "https://www.digitalocean.com/community/tutorials/how-to-create-a-cluster-of-docker-containers-with-docker-swarm-and-digitalocean-on-ubuntu-16-04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Upgrade Strategies\n",
    "\n",
    "\n",
    "  * _Blue-green_ ... Two identical environments, where only one is hot at any time\n",
    "  ![](https://opensource.com/sites/default/files/f1_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "  * _Canary_ ... Deploy to a small group first, then deploy to the rest\n",
    "  ![](https://opensource.com/sites/default/files/f3_0.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "  * _Rolling_ ... Deploy in rolling iterations\n",
    "  ![](https://opensource.com/sites/default/files/images/business-uploads/rolling2.gif)\n",
    "  \n",
    "See https://opensource.com/article/17/5/colorful-deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker Swarm: Rolling Updates\n",
    "\n",
    "  1. Stop the first task\n",
    "  2. Schedule update for the stopped task\n",
    "  3. Start the container for the updated task\n",
    "  4. If the update to a task returns RUNNING, wait for the specified delay period then start the next task\n",
    "  5. If, at any  me during the update, a task returns FAILED, pause the update\n",
    "\n",
    "PS: You need at least two replicas!\n",
    "\n",
    "\n",
    "See https://docs.docker.com/engine/swarm/swarm-tutorial/rolling-update/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating the Automation: Deploying the Swarm with Terraform\n",
    "\n",
    "#### Terraform\n",
    "- declarative scripting of complex cloud infrastructure\n",
    "- like vagrant but for the cloud\n",
    "- unlike Puppet, Chef which are for automating software on a single machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Your Task Until Next Week\n",
    "\n",
    "\n",
    "Either \n",
    "\n",
    "  * create a high-available setup with _hot_ and _standby_ server for your MiniTwit, or\n",
    "  * create a Docker Swarm cluster for your MiniTwit in which all components run as services.\n",
    "  \n",
    "  \n",
    "In both cases implement a rolling update strategy to your build chain."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
