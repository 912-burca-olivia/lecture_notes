
# Logging

Mircea Lungu (`mlun@itu.dk`)  

Lecture notes for: DevOps, Software Evolution and Software Maintenance @  [IT University of Copenhagen, Denmark](https://www.itu.dk)








## From Monitoring to Logging

*To think about...* 
- What are the types of problems detectable by monitoring? 
- Does monitoring help you understand why these problems occur? 















Monitoring does not explain *WHY* there was a problem

For the WHY there are other tools

- **Logging** (*main topic of today*) = understanding general kinds of problems 

- **Profiling** = understanding *performance* problems

- **Tracing** (*not today*) = understanding problems where the *sequence in which requests propagate* through distributed systems matter 





 
# Logging


## What is it? 

*def*(**Logging**) =  the **activity** of collecting and analyzing data generated by applications, infrastructure, and other components of a system.

def(**Logs**) =

- **streams** of 
- **aggregated**
- **time-ordered** events
- collected from **all running processes and backing services**

In server-based environments logs are commonly written to *a **logfile*** on disk. E.g., 

     cat /var/log/auth.log

Running the above command on an Internet-facing server should be a reminder of the importance of security.




## Why do it?

* **Understanding** (how is your system being used?)


* **Diagnosis** (of an actual problem)

  - What happened yesterday? 
  - Why is/was the service slow/down?
  - Were we under attack?


* **Audit trails**
  - Sometimes logs are legally required
  - Bitcoin = the most famous example? A big distributed log of all transactions.










## Challenges?

1. **Scalability of Analysis** = Searching through a deluge of log messages can be complicated 

2. **Compatibility of Formats** = Complex systems can generate logs in different formats 

3. **Storage Management** = Logging can result in very large data 


### e.g., The Tower of LogBabel... 

Every program uses a different format.

```
$ head -n 1 /var/log/auth.log

Mar 16 07:15:55 zeeguu-amsterdam sshd[29424]: Invalid user vultr from 144.217.243.216 
port 56450


$  head -n 1 /var/log/apache2/error.log

[Wed Mar 18 20:39:02.962354 2020] [wsgi:error] [pid 18:tid 140056344164096] 
[remote 212.187.36.136:57046] Session is retrived from cookies


$ head -n 1 /var/log/nginx/access.log

66.249.65.62 - - [06/Nov/2014:19:12:14 +0600] "GET /?q=%E0%A6%A6%E0%A7%8B%E0%A7%9F%E0%A6%BE 
HTTP/1.1" 200 4356 "-" "Mozilla/5.0 (compatible; Googlebot/2.1;)"


$ head -n 1 /var/log/system.log

Mar 18 21:25:16 Harlequin logd[85]: #DECODE failed to resolve UUID: [pc:0x7fff75485ac7 
ns:0x06 type:0x82 flags:0x8208 main:A52374C3-0F9D-3062-A636 pid:435]
```
























































# Architectures 

## Syslog 

Protocol

* Developed in 80s
* Standardizes **formatting** and **transmission** of logs in a network ([RFC 3164 (2001)](https://tools.ietf.org/html/rfc3164) [RFC 5424 (2009)](https://tools.ietf.org/html/rfc5424))
* For any system exchanging logs (although most popular in Linux)





### Entry Format

A syslog message is structured in a pre-defined format. Most essential elements are timestamp, application, level, and message. 

![](images/syslog_line.png)

### Facility Codes

0 - kern  = Kernel messages
1 - user = user-level messages
2 - mail = mail system
... [etc.](https://www.geeksforgeeks.org/what-is-syslog-server-and-its-working/)





### Log Levels

![](images/syslog_levels.png)



### Architectures

Syslog proposes a separation between the following roles: 

  - Originator = sender
  - Collector = responsible for gathering, receiving, and storing log messages
  - Relay = responsible with receiving syslog messages from multiple sources, possibly aggregating them or filtering them, and then forwarding them to one or more destinations

![](images/syslog_architectures.png)

Source: [RFC 5424 (2009)](https://tools.ietf.org/html/rfc5424)

Example of syslog configuration: 
```
cat /etc/rsyslog.conf 
```
## ELK

One of the most popular solutions at the moment. Acronym for

* ElasticSearch
* Logstash
* Kibana



![](images/ELK.png)





### E: ElasticSearch

Scalable Full Text Search

* Based on Apache Lucene
* Distributed & Replicated
* *Almost* real time full text search
* Stores logs in dedicated log indexes
* Based on JSON over HTTP


*Personal Story*: That time when we evaluated the peformance of [MySQL 8.0 Full Text Search](https://dev.mysql.com/doc/refman/8.0/en/fulltext-search.html) vs. ElasticSearch. And ES blows MySQL out of the water on this task. 




### L: Logstash

Java-Based Log Parser
- Converts from various log line formats to JSON
- Tails log files and emits events when a new log message is added
- Comes with a powerful pattern parsing plugin (Grok)

![](images/logstash_example_.png)


Challenges
- **Resource hungry**
- Not easy to configure + difficult to troubleshoot


#### Example: Parsing syslog format with Logstash


    input {
    	file {
    		path => "/Users/mircea/local/zeeguu/web.log"
    	}
    }

    filter {
    	grok {
    		match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{DATA:level} %{DATA:process} %{GREEDYDATA:log}" }
    	}
        date {
            match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
        }
    }

    output {
    	elasticsearch {
    		hosts => "elasticsearch:9200"
    		user => "elastic"
    		password => "changeme"
    	    index => "zeeguu_web"
    	}
    }


### K: Kibana

- Powerful visualization tool tailored for ElasticSearch
- Has its own query language: KQL

![](images/kibana-screenshot.png)



## Variations and Alternatives to ELK

There are many **variations** 
### EFLK: Filebeat for shipping logs

Filebeat = *Log Shipper*

- Addresses resource consumption of logstash
- Lightweight agents on different machines send logs to logstash
- Has special plugin for *docker* -- see your exercises for an example


![](images/FELK.png)



### EFK: Dropping the Logstash Alltogether

- Filebeat sends data straight to ElasticSearch 

- If you don't need to parse further the `@message` field

- In your exercises example !!!



### FRELK: With the Redis message broker

- Purpose: prevention of data loss

![](images/FRELK.png)





### .NET logging package + EK


> "My group ditched LogStash last year in part because it is slow, but also because .NET had a logging package that seamlessly integrated with ElasticSearch. So we basically just logged straight into ElasticSearch"
> (DevOps student from 2021)








### Promtail + Loki + Grafana

Promtail = **agent that ships the contents of local logs**

Loki = **log aggregation tool developed by Grafana labs** 

- Lightweight = Only indexes meta-data
- No distributed architecture for Loki (vs. ES)



![](images/loki-and-promtail.png)




### LNAV for visualizing multiple logfiles at once from terminal

LNAV = Log File Navigator ([lnav.org](https://lnav.org/))

- Basic search from the command line 
- Very little resources compared with ElasticSearch / Grafana
- Can aggregate live multiple files





































# Practical Principles

There are four main practical principles

## A process should not worry about storage

Don't hardcode to which logfile to write to.

Instead, each process should **write to its unbuffered stdout stream**.

Advantage is adaptability

* **In development**: the developer looks at the terminal

* **In deployment**: output from process is routed where needed 

* Different contexts result in different logfiles, e.g. cronjob







## A process should log only what is necessary

What's necessary for? 

- Apache
- Credit Suisse
- MiniTwit


Why? Because you avoid 

* **Redundancy**
	* e.g., you don't need log the web server accesses; they're already logged by your web server

* **Information Overload** 

* **Wasting disk space**

###### Personal Story: Not being able to login to the zeeguu server
The situation resulted from the following sequence of unfortunate events
- Logfile grows to multi-GB size in a few months
- Disk becomes full
- To the point of not being able to ssh into bash using ssh

Solution is to run a delete command  w/o even creating a terminal (e.g. `ssh user@server rm -rf /tmp`

### Architectural Tactic: Log Rotation

- Set a threshold of time / size 
- After which the data in the file is truncated / stored elsewhere



## Logging should be done *at the proper level*

Why? 
* Intention revealing
- Allows the user to control the amount of logging

Possible intention revealing classification of log levels

- DEBUG = anything that happens in the program => deploy to production carefully; ideally turn on only as needed
- INFO = actions that are **user-driven** and **meaningful for your system**
- NOTICE = thought out for production - notable events that are not an error
- WARN = events that might turn into an error 
- ERROR = every error situation
###### Personal Story: The Python library with very verbose logs!
I remember I was reusing this Python library that would generate a LOT of logs by default, so my own logs were drowning in their's. 




###### A Python example 
```python
import sys
import logging

logging.basicConfig(
        format="%(asctime)-15sZ %(levelname)s [%(module)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S.%f",
        level=logging.DEBUG,
        stream=sys.stdout
)

logging.debug("Got here!")
logging.info("Demo started.")
logging.warning("Ooops. Something went wrong!")
logging.error("Logging is broken in Javascript")
logging.critical("System will shut down")

```

    2022-03-22 11:05:23.fZ DEBUG [392630104] Got here!
    2022-03-22 11:05:23.fZ INFO [392630104] Demo started.
    2022-03-22 11:05:23.fZ WARNING [392630104] Ooops. Something went wrong!
    2022-03-22 11:05:23.fZ ERROR [392630104] Logging is broken in Javascript
    2022-03-22 11:05:23.fZ CRITICAL [392630104] System will shut down





## Logs  should be centralized

Why? Because having all the information in one place ... 
- Enables **correlation analysis**
- Is **more usable** than having to search through different files on different machines

Note: When you are running containers if you don't collect and ship the logs, they'll disappear when you destroy the container



















































# Ethical & Security Aspects

## Privacy and Security

* Do not log secrets in plain 

- Do not log user private data 
	- You might have to "GDPR-remove" them 

* Be aware of who has access to the logs

## Logging for Analytics?

You can use your own logging infra for analytics instead of relying on Google Analytics

Note: More logs => and more privacy concerns














# References 

- [Why Grafana is Good at Metrics and Not Logs](https://grafana.com/blog/2016/01/05/logs-and-metrics-and-graphs-oh-my/)
- [Loki vs Elasticsearch - Which tool to choose for Log Analytics?](https://signoz.io/blog/loki-vs-elasticsearch/)
- [What is a Syslog server and its working?](https://www.geeksforgeeks.org/what-is-syslog-server-and-its-working/)


# Further Considerations

## High-Resolution Logging 

- With  *sufficiently high-resolution* logging you can have a practical backup of the state of the database...

- *Binary logging* in the MySql context
	- Stream of events that modify the DB
	- Can be shipped across machines 

## Good to Know

- Docker - all logs can be found in `/var/lib/docker/containers/<container_id>`

- When using Docker containers - log files are lost when recreating containers

- ELK - you can reduce the memory allocated to it to about 700MB at the minimum

- At least one group succeeded in integrating Loki & Grafana instead of ELK in their setup

- Alternative to `docker logs `  is  [`docker attach`](https://docs.docker.com/engine/reference/commandline/attach/) 


## Logging vs. Crash Reporting
  
Similarity: often written to the same logfile

Difference: obviously not all logs are crashes

Example tool: Sentry



# Case Studies

Situations encountered in past iterations of this course. 

## Are my logs being sent to ElasticSearch?

Hi, we are using Serilog, Elasticsearch and kibana in our application for logging but kibana isn't showing any data. I'm not sure where in the process it is failing and the logs aren't being passed on. I've looked at countless guides and tutorial and our  configuration matches those but still haven't been able to get to work. Has anyone had any issues? or can offer help. thanks!

Try to debug it step by step. Is the data in ES?

If you know the name of your index, then you can `curl localhost:9200/nameofindex/_count` and you should see the number of "documents" (logs) in your case.

if you don't know the name of your index, then try to get all of them with something like `curl localhost:9200/_cat/indices`

![](../session_12/images/Pasted%20image%2020230508144501.png)


ah, now I see that you have two documents in each one of your indexes. every log message should be a document. you should definitely have more than 2 if your logs are being sent to ES. in fact, if you look at the name of those two indices, they're both named `.kibana*` - they are internal kibana indices; you have not succeeded in creating an index or sending any data to elastic search it seems. Probably better do the `docker logs` on the elasticsearch container to see whether you can learn something from that!




