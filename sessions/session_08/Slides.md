
# Logging

Mircea Lungu (`mlun@itu.dk`)  

Lecture notes for: DevOps, Software Evolution and Software Maintenance @  [IT University of Copenhagen, Denmark](https://www.itu.dk)








## From Monitoring to Logging

*To think about...* 
- What are the types of problems detectable by monitoring? 
- Does monitoring help you understand why these problems occur? 















Monitoring does not explain *WHY* there was a problem

For the WHY there are other tools

- **Logging** (*main topic of today*) = understanding general kinds of problems 

- **Profiling** = understanding *performance* problems

- **Tracing** (*not today*) = understanding problems where the *sequence in which requests propagate* through distributed systems matter 





 
# Logging


## What is it? 

*def*(**Logging**) =  the **activity** of collecting and analyzing data generated by applications, infrastructure, and other components of a system.

def(**Logs**) =

- **streams** of 
- **aggregated**
- **time-ordered** events
- collected from **all running processes and backing services**

In server-based environments logs are commonly written to *a **logfile*** on disk. E.g., 

     cat /var/log/auth.log

Running the above command on an Internet-facing server should be a reminder of the importance of security.




## Why do it?

There are three main reasons for logging: 

1. **Diagnosis** 

  - What happened yesterday when the user could not login? 
  - Why is the service slow?

2. **Understanding** 

 - How is your system being used?
 - Were we under attack last night?

3. **Audit trails**
  - Sometimes logs are legally required (e.g. banking)
  - Bitcoin = the most famous example? A big distributed log of all transactions.










## Challenges?

There are three main challenges
### **Scalability of Analysis** 
 Logs can quickly become very large and searching information in them can become tedious and difficult

### **Compatibility of Formats** 
 Complex systems can generate logs in different formats 
 
 E.g. three logfiles on the same system:
	```
	$ head -n 1 /var/log/auth.log
	$ head -n 1 /var/log/apache2/error.log
	$ head -n 1 /var/log/nginx/access.log```

### **Storage Management** 
 
 Logging can result in very large data that has to be managed.
###### Story: How to not be able to login to your sever anymore
The situation resulted from the following sequence of unfortunate events
- Logfile grows to multi-GB size in a few months
 - Disk becomes full
 - To the point of not even being able to ssh into bash 
 - Solution is to run a delete command  w/o even creating a terminal (e.g. `ssh user@server rm -rf /tmp`



























































# Architectures 

## Syslog 

Protocol

* Developed in 80s
* Standardizes **formatting** and **transmission** of logs in a network ([RFC 3164 (2001)](https://tools.ietf.org/html/rfc3164) [RFC 5424 (2009)](https://tools.ietf.org/html/rfc5424))
* Popular in Linux
* General - for any system exchanging logs 




### Formatting

A syslog message is structured in a pre-defined format. Most essential elements are timestamp, application, level, and message. 

![](images/syslog_line.png)

### Facility Codes

0 - kern  = Kernel messages
1 - user = user-level messages
2 - mail = mail system
... [etc.](https://www.geeksforgeeks.org/what-is-syslog-server-and-its-working/)





### Log Levels

![](images/syslog_levels.png)



### Architectures for Log Transmission

Syslog proposes a separation between the following roles 

  - **Originator** = sender
  - **Collector** = responsible for gathering, receiving, and storing log messages
  - **Relay** = responsible with receiving syslog messages from multiple sources, possibly aggregating them or filtering them, and then forwarding them to one or more destinations

![](images/syslog_architectures.png)

Source: [RFC 5424 (2009)](https://tools.ietf.org/html/rfc5424)

Example of syslog configuration: 
```
cat /etc/rsyslog.conf 
```
## ELK

One of the most popular solutions at the moment. 

Acronym for

* ElasticSearch = Scalable full text search DB
* Logstash = Java-based log parser
* Kibana = Visualization tool tailored for ElasticSearch


![](images/ELK.png)





### ElasticSearch

Database 
* *Almost* real time full text search
* Stores logs in dedicated log indexes
* Distributed & replicated
* JSON over HTTP

*Story*: That time when we evaluated the performance of [MySQL 8.0 Full Text Search](https://dev.mysql.com/doc/refman/8.0/en/fulltext-search.html) vs. ElasticSearch. And ES blows MySQL out of the water on this task. 




### Logstash

Java-Based Log Parser which ... 

- Converts from various log line formats to JSON
- *Tails* log files and emits events when a new log message is added
- Comes with a pattern parsing plugin: Grok

![](images/logstash_example_.png)


Challenges
- **Resource hungry**
- Not easy to configure + difficult to troubleshoot




### Kibana

- Visualization tool tailored for ElasticSearch
- Has its own query language: KQL

![](images/kibana-screenshot.png)



## Variations and Alternatives to ELK

There are many **variations** where one component in ELK is replaced with another or new components are introduced. 

### EFLK: Filebeat for shipping logs

Filebeat = *Log Shipper*

- Addresses resource consumption of logstash
- Lightweight agents on different machines send logs to logstash
- Has special plugin for *docker* -- see your exercises for an example


![](images/FELK.png)



### EFK: Dropping the Logstash Alltogether

- Filebeat sends data straight to ElasticSearch 

- If you don't need to parse further the `@message` field

- In your exercises example



### FRELK: With the Redis message broker

Redis = in memory data structure that can be used as DB, cache, and message broker

Purpose: prevention of data loss. Can you explain how?

![](images/FRELK.png)











### Promtail + Loki + Grafana

Promtail = **agent that ships the contents of local logs**

Loki = **log aggregation tool developed by Grafana labs** 

- Lightweight = Only indexes meta-data
- No distributed architecture for Loki (vs. ES)



![](images/loki-and-promtail.png)











# Practical Principles

There are four main practical principles

## A process should not worry about storage

Don't hardcode to which logfile to write to.

Instead, each process should **write to its unbuffered stdout stream**.

Advantage is adaptability

* **In development**: the developer looks at the terminal

* **In deployment**: output from process is routed where needed 

* Different contexts result in different logfiles, e.g. cronjob







## A process should log only what is necessary

What's necessary for? 

- Apache
- Credit Suisse
- MiniTwit


Why? Because you avoid 

* **Redundancy**
	* e.g., you don't need log the web server accesses; they're already logged by your web server

* **Information Overload** 

* **Wasting disk space**
### Architectural Tactic: Log Rotation

- Set a threshold of time / size 
- After which the data in the file is truncated / stored elsewhere



## Logging should be done *at the proper level*

Why? 
* Intention revealing
- Allows the user to control the amount of logging

Possible intention revealing classification of log levels

- DEBUG = anything that happens in the program => deploy to production carefully; ideally turn on only as needed
- INFO = actions that are **user-driven** and **meaningful for your system**
- NOTICE = thought out for production - notable events that are not an error
- WARN = events that might turn into an error 
- ERROR = every error situation
###### Personal Story: The Python library with very verbose logs!
I remember I was reusing this Python library that would generate a LOT of logs by default, so my own logs were drowning in their's. 




###### A Python example 
```python
import sys
import logging

logging.basicConfig(
        format="%(asctime)-15sZ %(levelname)s [%(module)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S.%f",
        level=logging.DEBUG,
        stream=sys.stdout
)

logging.debug("Got here!")
logging.info("Demo started.")
logging.warning("Ooops. Something went wrong!")
logging.error("Logging is broken in Javascript")
logging.critical("System will shut down")

```

    2022-03-22 11:05:23.fZ DEBUG [392630104] Got here!
    2022-03-22 11:05:23.fZ INFO [392630104] Demo started.
    2022-03-22 11:05:23.fZ WARNING [392630104] Ooops. Something went wrong!
    2022-03-22 11:05:23.fZ ERROR [392630104] Logging is broken in Javascript
    2022-03-22 11:05:23.fZ CRITICAL [392630104] System will shut down





## Logs  should be centralized

Why? Because having all the information in one place ... 
- Enables **correlation analysis**
- Is **more usable** than having to search through different files on different machines

Note: When you are running containers if you don't collect and ship the logs, they'll disappear when you destroy the container



















































# Ethical & Security Aspects

## Privacy and Security

* Do not log secrets in plain 

- Do not log user private data 
	- You might have to "GDPR-remove" them 

* Be aware of who has access to the logs

## Logging for Analytics?

You can use your own logging infra for analytics instead of relying on Google Analytics

Note: More logs => and more privacy concerns














# References 

- [Why Grafana is Good at Metrics and Not Logs](https://grafana.com/blog/2016/01/05/logs-and-metrics-and-graphs-oh-my/)
- [Loki vs Elasticsearch - Which tool to choose for Log Analytics?](https://signoz.io/blog/loki-vs-elasticsearch/)
- [What is a Syslog server and its working?](https://www.geeksforgeeks.org/what-is-syslog-server-and-its-working/)


# Further Considerations

## High-Resolution Logging 

- With  *sufficiently high-resolution* logging you can have a practical backup of the state of the database...

- *Binary logging* in the MySql context
	- Stream of events that modify the DB
	- Can be shipped across machines 

## Good to Know

- Docker - all logs can be found in `/var/lib/docker/containers/<container_id>`

- When using Docker containers - log files are lost when recreating containers

- ELK - you can reduce the memory allocated to it to about 700MB at the minimum

- At least one group succeeded in integrating Loki & Grafana instead of ELK in their setup

- Alternative to `docker logs `  is  [`docker attach`](https://docs.docker.com/engine/reference/commandline/attach/) 


## Logging vs. Crash Reporting
  
Similarity: often written to the same logfile

Difference: obviously not all logs are crashes

Example tool: Sentry



# Case Studies

Situations encountered in past iterations of this course. 

## .NET logging package + EK

> "My group ditched LogStash last year in part because it is slow, but also because .NET had a logging package that seamlessly integrated with ElasticSearch. So we basically just logged straight into ElasticSearch"
> (DevOps student from 2021)

## Are my logs being sent to ElasticSearch?

Hi, we are using Serilog, Elasticsearch and kibana in our application for logging but kibana isn't showing any data. I'm not sure where in the process it is failing and the logs aren't being passed on. I've looked at countless guides and tutorial and our  configuration matches those but still haven't been able to get to work. Has anyone had any issues? or can offer help. thanks!

Try to debug it step by step. Is the data in ES?

If you know the name of your index, then you can `curl localhost:9200/nameofindex/_count` and you should see the number of "documents" (logs) in your case.

if you don't know the name of your index, then try to get all of them with something like `curl localhost:9200/_cat/indices`

![](../session_12/images/Pasted%20image%2020230508144501.png)


ah, now I see that you have two documents in each one of your indexes. every log message should be a document. you should definitely have more than 2 if your logs are being sent to ES. in fact, if you look at the name of those two indices, they're both named `.kibana*` - they are internal kibana indices; you have not succeeded in creating an index or sending any data to elastic search it seems. Probably better do the `docker logs` on the elasticsearch container to see whether you can learn something from that!

## Example Configuration of Logstash

I used this for one of my projects 

    input {
    	file {
    		path => "/Users/mircea/local/zeeguu/web.log"
    	}
    }

    filter {
    	grok {
    		match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{DATA:level} %{DATA:process} %{GREEDYDATA:log}" }
    	}
        date {
            match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
        }
    }

    output {
    	elasticsearch {
    		hosts => "elasticsearch:9200"
    		user => "elastic"
    		password => "changeme"
    	    index => "zeeguu_web"
    	}
    }

## LNAV for visualizing multiple logfiles at once from terminal

LNAV = Log File Navigator ([lnav.org](https://lnav.org/))

- Terminal based (as opposed to web-based)
- Can aggregate live multiple files
- Supports basic search from the command line 
- Very little resources compared with ElasticSearch / Grafana

